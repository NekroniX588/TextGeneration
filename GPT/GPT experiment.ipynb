{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(hypotheses, references, tokenize=\"13a\"):\n",
    "    \"\"\"\n",
    "    Raw corpus BLEU from sacrebleu (without tokenization)\n",
    "    :param hypotheses: list of hypotheses (strings)\n",
    "    :param references: list of references (strings)\n",
    "    :param tokenize: one of {'none', '13a', 'intl', 'zh', 'ja-mecab'}\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sacrebleu.corpus_bleu(sys_stream=hypotheses,\n",
    "                                 ref_streams=[references],\n",
    "                                 tokenize=tokenize).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-06 20:33:49--  https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10474 (10K) [text/plain]\n",
      "Saving to: 'generate_transformers.py'\n",
      "\n",
      "generate_transforme 100%[===================>]  10.23K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2021-04-06 20:33:50 (6.01 MB/s) - 'generate_transformers.py' saved [10474/10474]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-04-06 20:33:51--  https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/pretrain_transformers.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 34150 (33K) [text/plain]\n",
      "Saving to: 'pretrain_transformers.py'\n",
      "\n",
      "pretrain_transforme 100%[===================>]  33.35K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-04-06 20:33:51 (572 KB/s) - 'pretrain_transformers.py' saved [34150/34150]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/pretrain_transformers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_HOME=/usr/local/cuda\n",
    "#!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_transformers import *\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.model_type = 'gpt2'\n",
    "        self.model_name_or_path = 'sberbank-ai/rugpt3large_based_on_gpt2'\n",
    "\n",
    "        self.prompt = ''\n",
    "        self.length = 50\n",
    "        self.stop_token = '</s>'\n",
    "\n",
    "        self.k = 5\n",
    "        self.p = .95\n",
    "        self.temperature = 1\n",
    "\n",
    "        self.repetition_penalty = 1\n",
    "        self.num_return_sequences = 1\n",
    "\n",
    "        self.device='cuda'\n",
    "        self.seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(prompt_text, args, delimiter='>>>'):\n",
    "    args.prompt_text = prompt_text\n",
    "\n",
    "    \n",
    "    if prompt_text.endswith('.txt'):\n",
    "      with open(prompt_text, 'r') as f:\n",
    "        prompt_text = f.read()\n",
    "\n",
    "    # print(f'Input:\\n{prompt_text}\\n')\n",
    "    \n",
    "    encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    encoded_prompt = encoded_prompt.to(args.device)\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=encoded_prompt,\n",
    "        max_length=args.length + len(encoded_prompt[0]),\n",
    "        temperature=args.temperature,\n",
    "        top_k=args.k,\n",
    "        top_p=args.p,\n",
    "        repetition_penalty=args.repetition_penalty,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=args.num_return_sequences,\n",
    "    )\n",
    "\n",
    "    if len(output_sequences.shape) > 2:\n",
    "            output_sequences.squeeze_()\n",
    "\n",
    "    generated_sequences = []\n",
    "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "        text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "        text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "\n",
    "        if delimiter in text:\n",
    "            text = text.split(delimiter)[0].rstrip()\n",
    "        else:\n",
    "            text = text.split('\\n')[0].rstrip()\n",
    "\n",
    "        generated_sequences.append(text)\n",
    "        # print(f'[{generated_sequence_idx}]ruGPT:\\n{prompt_text.split('\\n')[-1] + text}')\n",
    "\n",
    "    return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_text\n",
      "  Downloading tensorflow_text-2.6.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow<2.7,>=2.6.0\n",
      "  Downloading tensorflow-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (458.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 458.4 MB 45 kB/s  eta 0:00:012\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.8.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six~=1.15.0\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting numpy~=1.19.2\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 10.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.6\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.7,>=2.6.0->tensorflow_text) (3.17.3)\n",
      "Collecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 2.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 2.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting wheel~=0.35\n",
      "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.41.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 18.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.14.1-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 19.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator~=2.6\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "Collecting keras~=2.6\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 13.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 17.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (45.2.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "\u001b[K     |████████████████████████████████| 152 kB 19.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.26.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7,>=2.6.0->tensorflow_text) (1.26.6)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 4.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "\u001b[K     |████████████████████████████████| 146 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, clang, wrapt\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=51a4c7d74982eaf3e5fda1ca390e5c93aaa1d88deee892923e15e376677937ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=73ae98a227442cf687d323a89ff30dff28723e5837619ff5f10d0e314dafb4d1\n",
      "  Stored in directory: /root/.cache/pip/wheels/f1/60/77/22b9b5887bd47801796a856f47650d9789c74dc3161a26d608\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=78521 sha256=590ae3db1446c828955e5a6bc4c2f274d7b1acd819a85820d9611294ab8716ef\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor clang wrapt\n",
      "Installing collected packages: six, numpy, tensorboard-data-server, wheel, absl-py, tensorboard-plugin-wit, markdown, pyasn1, rsa, pyasn1-modules, cachetools, google-auth, grpcio, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, termcolor, google-pasta, opt-einsum, gast, flatbuffers, h5py, tensorflow-estimator, clang, typing-extensions, keras-preprocessing, wrapt, keras, astunparse, tensorflow, tensorflow-hub, tensorflow-text\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.1\n",
      "    Uninstalling numpy-1.21.1:\n",
      "      Successfully uninstalled numpy-1.21.1\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.34.2\n",
      "    Not uninstalling wheel at /usr/lib/python3/dist-packages, outside environment /usr\n",
      "    Can't uninstall 'wheel'. No files were found to uninstall.\n",
      "Successfully installed absl-py-0.14.1 astunparse-1.6.3 cachetools-4.2.4 clang-5.0 flatbuffers-1.12 gast-0.4.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.41.0 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 oauthlib-3.1.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 six-1.15.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.6.0 tensorflow-estimator-2.6.0 tensorflow-hub-0.12.0 tensorflow-text-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wheel-0.37.0 wrapt-1.12.1\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/21/2021 11:04:13 - INFO - absl -   Using /tmp/tfhub_modules to cache modules.\n",
      "2021-10-21 11:04:13.518890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-21 11:04:13.520201: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-10-21 11:04:13.520269: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-10-21 11:04:13.520338: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-10-21 11:04:13.520400: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-10-21 11:04:13.520458: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-10-21 11:04:13.520515: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-10-21 11:04:13.520585: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2021-10-21 11:04:13.520595: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-10-21 11:04:13.520821: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-21 11:04:14.587759: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "def compute_use(target_comment, generated_comments):\n",
    "    target_comment = embed(list([target_comment]))\n",
    "    generated_comments = list(map(embed, generated_comments))\n",
    "\n",
    "    return [np.inner(target_comment, gc)[0][0] for gc in generated_comments]\n",
    "\n",
    "\n",
    "def compare_results(source_comment, target_comment, generated_comments, scores):\n",
    "    print(f'Toxic : {source_comment}')\n",
    "    print(f'Polite: {target_comment}\\n')\n",
    "\n",
    "    print(f'Score  Generated Comment')\n",
    "    for i in np.argsort(scores):\n",
    "        print(np.round(scores[i], 3), generated_comments[i])\n",
    "\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/21/2021 07:23:50 - INFO - filelock -   Lock 140134280355024 acquired on /root/.cache/huggingface/transformers/3a4aea75af518e10b02aac733b25c92d979cec74d3b48edb877e13d5e4d4792f.aa4141df4e4cfca5435e8aa371aea7f575eb2d0767c1395e0d4cd6209796a705.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2546c06567c7480b9f9a05a2d2e81295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/21/2021 07:23:51 - INFO - filelock -   Lock 140134280355024 released on /root/.cache/huggingface/transformers/3a4aea75af518e10b02aac733b25c92d979cec74d3b48edb877e13d5e4d4792f.aa4141df4e4cfca5435e8aa371aea7f575eb2d0767c1395e0d4cd6209796a705.lock\n",
      "10/21/2021 07:23:53 - INFO - filelock -   Lock 140133714131744 acquired on /root/.cache/huggingface/transformers/7cf248f6c39196b677fb5b9b9ee8da3ded29f363018f4ccb1d0605721c719f4c.75e651cd6468a93822a2ca422a07b480dacd0c2d13ac194fdf771f768e6a8447.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1665245341044bd388a5a36b2d618f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/21/2021 07:24:15 - INFO - filelock -   Lock 140133714131744 released on /root/.cache/huggingface/transformers/7cf248f6c39196b677fb5b9b9ee8da3ded29f363018f4ccb1d0605721c719f4c.75e651cd6468a93822a2ca422a07b480dacd0c2d13ac194fdf771f768e6a8447.lock\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3132/1959338045.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                 \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                 resolved_archive_file = cached_path(\n\u001b[0m\u001b[1;32m   1298\u001b[0m                     \u001b[0marchive_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                     \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1403\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1666\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTSET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m     )\n\u001b[0;32m-> 1525\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m                 if (\n\u001b[1;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "\n",
    "model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
    "model = model_class.from_pretrained(args.model_name_or_path)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.length = 200\n",
    "args.num_return_sequences = 10\n",
    "\n",
    "args.k = 3\n",
    "args.p = .5\n",
    "args.temperature = 5\n",
    "\n",
    "\n",
    "args.length = adjust_length_to_model(args.length, max_sequence_length=model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimiter = \" >>>> \"\n",
    "\n",
    "train_ky = []\n",
    "train_ru = []\n",
    "train_data = []\n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/train.tok.be', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        train_ky.append(line)   \n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/train.tok.ru', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        train_ru.append(line)\n",
    "for i in range(len(train_ky)):\n",
    "    line = train_ky[i] + delimiter + train_ru[i]\n",
    "    train_data.append(line)\n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/gpt_data/train.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(train_data[:100000]))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "valid_ky = []\n",
    "valid_ru = []\n",
    "valid_data = []\n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/test.tok.be', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        valid_ky.append(line)   \n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/test.tok.ru', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        valid_ru.append(line)\n",
    "for i in range(len(valid_ky)):\n",
    "    line = valid_ky[i] + delimiter + valid_ru[i]\n",
    "    valid_data.append(line)\n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/gpt_data/valid.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(valid_data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/11/2021 14:10:07 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/auto/modeling_auto.py:806: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "10/11/2021 14:10:25 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=2048, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=False, do_train=True, eval_all_checkpoints=False, eval_data_file='/mnt/DATA2/grashchenkov/Checheny-Russki/sah_ru_data/gpt_data/valid.txt', evaluate_during_training=False, fp16=True, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='sberbank-ai/rugpt3medium_based_on_gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=25.0, output_dir='gpt_translation_model', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=5, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='/mnt/DATA2/grashchenkov/Checheny-Russki/sah_ru_data/gpt_data/train.txt', warmup_steps=0, weight_decay=0.01)\n",
      "10/11/2021 14:10:25 - INFO - __main__ -   Creating features from dataset file at /mnt/DATA2/grashchenkov/Checheny-Russki/sah_ru_data/gpt_data/train.txt\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_transformers.py\", line 252, in train\n",
      "    from apex import amp\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/apex/__init__.py\", line 13, in <module>\n",
      "    from pyramid.session import UnencryptedCookieSessionFactoryConfig\n",
      "ImportError: cannot import name 'UnencryptedCookieSessionFactoryConfig' from 'pyramid.session' (unknown location)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pretrain_transformers.py\", line 782, in <module>\n",
      "    main()\n",
      "  File \"pretrain_transformers.py\", line 731, in main\n",
      "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
      "  File \"pretrain_transformers.py\", line 254, in train\n",
      "    raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
      "ImportError: Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\n"
     ]
    }
   ],
   "source": [
    "!python3 pretrain_transformers.py \\\n",
    "    --line_by_line \\\n",
    "    --output_dir=gpt_translation_model \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=sberbank-ai/rugpt3medium_based_on_gpt2 \\\n",
    "    --do_train \\\n",
    "    --train_data_file=/mnt/DATA2/grashchenkov/Checheny-Russki/sah_ru_data/gpt_data/train.txt \\\n",
    "    --fp16 \\\n",
    "    --eval_data_file=/mnt/DATA2/grashchenkov/Checheny-Russki/sah_ru_data/gpt_data/valid.txt \\\n",
    "    --per_gpu_train_batch_size 5 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --num_train_epochs 25 \\\n",
    "    --block_size 2048 \\\n",
    "    --overwrite_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50258, 1024)\n",
       "    (wpe): Embedding(2048, 1024)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): GPT2Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50258, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = Args()\n",
    "args.model_name_or_path = 'gpt_translation_model'\n",
    "\n",
    "model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_name_or_path)\n",
    "model = model_class.from_pretrained(args.model_name_or_path)\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.length = 50\n",
    "args.num_return_sequences = 3\n",
    "\n",
    "args.k = 3\n",
    "args.p = .5\n",
    "args.temperature = 5.2\n",
    "\n",
    "\n",
    "args.length = adjust_length_to_model(args.length, max_sequence_length=model.config.max_position_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "delimiter = '>>>>'\n",
    "\n",
    "generated_sequences = generate_sequences('Мне пара ісці спаць .' + f' {delimiter} ', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Мне пора идти спать.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad',\n",
       " ' Мне пора идти спать.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad',\n",
       " ' Мне пора идти спать.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ky = []\n",
    "test_ru = []\n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/test.tok.be', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        test_ky.append(line)   \n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/test.tok.ru', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        test_ru.append(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/255 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  0%|          | 1/255 [00:00<03:11,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 2/255 [00:01<03:32,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  1%|          | 3/255 [00:02<03:18,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  2%|▏         | 4/255 [00:03<03:10,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  2%|▏         | 5/255 [00:03<03:06,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  2%|▏         | 6/255 [00:04<03:05,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|▎         | 7/255 [00:05<03:03,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|▎         | 8/255 [00:05<03:00,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▎         | 9/255 [00:06<02:59,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 10/255 [00:07<03:00,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  4%|▍         | 11/255 [00:08<02:57,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  5%|▍         | 12/255 [00:08<02:57,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  5%|▌         | 13/255 [00:09<02:56,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  5%|▌         | 14/255 [00:10<02:55,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  6%|▌         | 15/255 [00:11<02:54,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  6%|▋         | 16/255 [00:11<02:56,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|▋         | 17/255 [00:12<02:53,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|▋         | 18/255 [00:13<02:52,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|▋         | 19/255 [00:14<02:50,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 20/255 [00:14<02:49,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  8%|▊         | 21/255 [00:15<02:53,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  9%|▊         | 22/255 [00:16<02:55,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  9%|▉         | 23/255 [00:17<02:52,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  9%|▉         | 24/255 [00:17<02:50,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 10%|▉         | 25/255 [00:18<02:48,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 10%|█         | 26/255 [00:19<02:44,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 11%|█         | 27/255 [00:19<02:41,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 11%|█         | 28/255 [00:20<02:38,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 11%|█▏        | 29/255 [00:21<02:38,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▏        | 30/255 [00:21<02:38,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 12%|█▏        | 31/255 [00:22<02:37,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|█▎        | 32/255 [00:23<02:41,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|█▎        | 33/255 [00:24<02:41,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|█▎        | 34/255 [00:24<02:40,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 14%|█▎        | 35/255 [00:25<02:44,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 14%|█▍        | 36/255 [00:26<02:54,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 15%|█▍        | 37/255 [00:27<02:48,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 15%|█▍        | 38/255 [00:28<02:44,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 15%|█▌        | 39/255 [00:28<02:42,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 16%|█▌        | 40/255 [00:29<02:40,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 16%|█▌        | 41/255 [00:30<02:37,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 16%|█▋        | 42/255 [00:30<02:35,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 43/255 [00:31<02:33,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|█▋        | 44/255 [00:32<02:31,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 18%|█▊        | 45/255 [00:33<02:31,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 18%|█▊        | 46/255 [00:33<02:31,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 18%|█▊        | 47/255 [00:34<02:30,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 19%|█▉        | 48/255 [00:35<02:29,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 19%|█▉        | 49/255 [00:35<02:27,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|█▉        | 50/255 [00:36<02:26,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|██        | 51/255 [00:37<02:25,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|██        | 52/255 [00:38<02:24,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 53/255 [00:38<02:24,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 21%|██        | 54/255 [00:39<02:23,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 22%|██▏       | 55/255 [00:40<02:24,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 22%|██▏       | 56/255 [00:40<02:24,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 22%|██▏       | 57/255 [00:41<02:23,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|██▎       | 58/255 [00:42<02:23,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|██▎       | 59/255 [00:43<02:24,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 24%|██▎       | 60/255 [00:43<02:27,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 24%|██▍       | 61/255 [00:44<02:23,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 24%|██▍       | 62/255 [00:45<02:24,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▍       | 63/255 [00:46<02:24,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 64/255 [00:46<02:23,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 25%|██▌       | 65/255 [00:47<02:20,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 26%|██▌       | 66/255 [00:48<02:18,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 26%|██▋       | 67/255 [00:49<02:16,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|██▋       | 68/255 [00:49<02:14,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|██▋       | 69/255 [00:50<02:13,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|██▋       | 70/255 [00:51<02:12,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 28%|██▊       | 71/255 [00:51<02:11,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 28%|██▊       | 72/255 [00:52<02:13,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▊       | 73/255 [00:53<02:11,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 74/255 [00:54<02:10,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 29%|██▉       | 75/255 [00:54<02:09,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|██▉       | 76/255 [00:55<02:15,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|███       | 77/255 [00:56<02:12,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|███       | 78/255 [00:57<02:09,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|███       | 79/255 [00:57<02:08,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 31%|███▏      | 80/255 [00:58<02:06,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 32%|███▏      | 81/255 [00:59<02:05,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 32%|███▏      | 82/255 [00:59<02:04,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 83/255 [01:00<02:04,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 84/255 [01:01<02:04,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|███▎      | 85/255 [01:02<02:04,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 34%|███▎      | 86/255 [01:02<02:07,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 34%|███▍      | 87/255 [01:03<02:06,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 35%|███▍      | 88/255 [01:04<02:04,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 35%|███▍      | 89/255 [01:05<02:02,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 35%|███▌      | 90/255 [01:05<02:01,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 36%|███▌      | 91/255 [01:06<02:02,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 36%|███▌      | 92/255 [01:07<02:00,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 36%|███▋      | 93/255 [01:08<01:59,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|███▋      | 94/255 [01:08<02:01,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|███▋      | 95/255 [01:09<02:04,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 96/255 [01:10<02:02,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 97/255 [01:11<02:02,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 38%|███▊      | 98/255 [01:12<02:00,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 39%|███▉      | 99/255 [01:12<01:59,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 39%|███▉      | 100/255 [01:13<02:03,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|███▉      | 101/255 [01:14<01:57,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|████      | 102/255 [01:15<01:54,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|████      | 103/255 [01:15<01:54,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 41%|████      | 104/255 [01:16<01:53,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 41%|████      | 105/255 [01:17<01:52,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 106/255 [01:18<01:54,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 107/255 [01:18<01:50,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 42%|████▏     | 108/255 [01:19<01:47,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 43%|████▎     | 109/255 [01:20<01:45,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 43%|████▎     | 110/255 [01:20<01:43,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|████▎     | 111/255 [01:21<01:47,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|████▍     | 112/255 [01:22<01:54,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 44%|████▍     | 113/255 [01:23<01:51,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 45%|████▍     | 114/255 [01:24<01:47,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 45%|████▌     | 115/255 [01:24<01:43,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 45%|████▌     | 116/255 [01:25<01:40,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▌     | 117/255 [01:26<01:39,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 46%|████▋     | 118/255 [01:26<01:37,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|████▋     | 119/255 [01:27<01:36,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|████▋     | 120/255 [01:28<01:37,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|████▋     | 121/255 [01:29<01:35,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 48%|████▊     | 122/255 [01:29<01:34,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 48%|████▊     | 123/255 [01:30<01:32,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 49%|████▊     | 124/255 [01:31<01:31,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 49%|████▉     | 125/255 [01:31<01:30,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 49%|████▉     | 126/255 [01:32<01:30,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|████▉     | 127/255 [01:33<01:30,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|█████     | 128/255 [01:34<01:30,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 51%|█████     | 129/255 [01:34<01:30,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 51%|█████     | 130/255 [01:35<01:30,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 51%|█████▏    | 131/255 [01:36<01:29,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 52%|█████▏    | 132/255 [01:36<01:31,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 52%|█████▏    | 133/255 [01:37<01:32,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|█████▎    | 134/255 [01:38<01:31,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|█████▎    | 135/255 [01:39<01:32,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|█████▎    | 136/255 [01:40<01:32,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▎    | 137/255 [01:41<01:38,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 54%|█████▍    | 138/255 [01:41<01:38,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 55%|█████▍    | 139/255 [01:42<01:33,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 55%|█████▍    | 140/255 [01:43<01:34,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 55%|█████▌    | 141/255 [01:44<01:29,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|█████▌    | 142/255 [01:44<01:27,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|█████▌    | 143/255 [01:45<01:25,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 56%|█████▋    | 144/255 [01:46<01:25,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 57%|█████▋    | 145/255 [01:47<01:25,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 57%|█████▋    | 146/255 [01:48<01:23,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 147/255 [01:48<01:20,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 148/255 [01:49<01:19,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 58%|█████▊    | 149/255 [01:50<01:18,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 59%|█████▉    | 150/255 [01:50<01:16,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 59%|█████▉    | 151/255 [01:51<01:18,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|█████▉    | 152/255 [01:52<01:22,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|██████    | 153/255 [01:53<01:21,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|██████    | 154/255 [01:54<01:18,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 61%|██████    | 155/255 [01:54<01:15,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 61%|██████    | 156/255 [01:55<01:13,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▏   | 157/255 [01:56<01:11,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▏   | 158/255 [01:57<01:10,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 62%|██████▏   | 159/255 [01:57<01:09,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|██████▎   | 160/255 [01:58<01:08,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|██████▎   | 161/255 [01:59<01:08,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 64%|██████▎   | 162/255 [01:59<01:06,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 64%|██████▍   | 163/255 [02:00<01:05,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 64%|██████▍   | 164/255 [02:01<01:04,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 65%|██████▍   | 165/255 [02:01<01:03,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 65%|██████▌   | 166/255 [02:02<01:02,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 65%|██████▌   | 167/255 [02:03<01:02,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 66%|██████▌   | 168/255 [02:04<01:01,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 66%|██████▋   | 169/255 [02:04<01:00,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 170/255 [02:05<00:59,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 171/255 [02:06<00:58,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|██████▋   | 172/255 [02:06<00:58,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 68%|██████▊   | 173/255 [02:07<00:57,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 68%|██████▊   | 174/255 [02:08<00:59,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|██████▊   | 175/255 [02:09<00:59,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|██████▉   | 176/255 [02:09<00:59,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 69%|██████▉   | 177/255 [02:10<00:59,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|██████▉   | 178/255 [02:11<00:58,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|███████   | 179/255 [02:12<00:56,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 180/255 [02:12<00:55,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████   | 181/255 [02:13<00:53,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 71%|███████▏  | 182/255 [02:14<00:52,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 72%|███████▏  | 183/255 [02:15<00:51,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 72%|███████▏  | 184/255 [02:15<00:50,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|███████▎  | 185/255 [02:16<00:50,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|███████▎  | 186/255 [02:17<00:49,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|███████▎  | 187/255 [02:18<00:51,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 74%|███████▎  | 188/255 [02:18<00:50,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 74%|███████▍  | 189/255 [02:19<00:48,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▍  | 190/255 [02:20<00:47,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▍  | 191/255 [02:20<00:46,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 75%|███████▌  | 192/255 [02:21<00:45,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 76%|███████▌  | 193/255 [02:22<00:45,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 76%|███████▌  | 194/255 [02:23<00:49,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 76%|███████▋  | 195/255 [02:24<00:52,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|███████▋  | 196/255 [02:25<00:48,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|███████▋  | 197/255 [02:25<00:46,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 78%|███████▊  | 198/255 [02:26<00:44,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 78%|███████▊  | 199/255 [02:27<00:42,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 78%|███████▊  | 200/255 [02:27<00:41,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 201/255 [02:28<00:39,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 79%|███████▉  | 202/255 [02:29<00:38,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|███████▉  | 203/255 [02:30<00:37,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|████████  | 204/255 [02:30<00:36,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|████████  | 205/255 [02:31<00:36,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 81%|████████  | 206/255 [02:32<00:37,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 81%|████████  | 207/255 [02:33<00:38,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 82%|████████▏ | 208/255 [02:34<00:40,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 82%|████████▏ | 209/255 [02:35<00:39,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 82%|████████▏ | 210/255 [02:35<00:37,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 211/255 [02:36<00:35,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|████████▎ | 212/255 [02:37<00:33,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 84%|████████▎ | 213/255 [02:38<00:34,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 84%|████████▍ | 214/255 [02:39<00:33,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 84%|████████▍ | 215/255 [02:39<00:32,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 85%|████████▍ | 216/255 [02:40<00:31,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 85%|████████▌ | 217/255 [02:41<00:29,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 85%|████████▌ | 218/255 [02:42<00:29,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 86%|████████▌ | 219/255 [02:43<00:28,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 86%|████████▋ | 220/255 [02:43<00:27,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|████████▋ | 221/255 [02:44<00:25,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|████████▋ | 222/255 [02:45<00:24,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|████████▋ | 223/255 [02:45<00:23,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 224/255 [02:46<00:22,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 88%|████████▊ | 225/255 [02:47<00:21,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 89%|████████▊ | 226/255 [02:47<00:20,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 89%|████████▉ | 227/255 [02:48<00:20,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 89%|████████▉ | 228/255 [02:49<00:19,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 90%|████████▉ | 229/255 [02:50<00:18,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 90%|█████████ | 230/255 [02:50<00:17,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 91%|█████████ | 231/255 [02:51<00:16,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 91%|█████████ | 232/255 [02:52<00:16,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 91%|█████████▏| 233/255 [02:52<00:15,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 234/255 [02:53<00:14,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 92%|█████████▏| 235/255 [02:54<00:14,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|█████████▎| 236/255 [02:55<00:13,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|█████████▎| 237/255 [02:55<00:12,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|█████████▎| 238/255 [02:56<00:12,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|█████████▎| 239/255 [02:57<00:11,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 94%|█████████▍| 240/255 [02:57<00:10,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 95%|█████████▍| 241/255 [02:58<00:09,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 95%|█████████▍| 242/255 [02:59<00:09,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 95%|█████████▌| 243/255 [03:00<00:08,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 244/255 [03:00<00:07,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▌| 245/255 [03:01<00:07,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 96%|█████████▋| 246/255 [03:02<00:06,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|█████████▋| 247/255 [03:03<00:06,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|█████████▋| 248/255 [03:03<00:05,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 98%|█████████▊| 249/255 [03:04<00:04,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 98%|█████████▊| 250/255 [03:05<00:03,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 98%|█████████▊| 251/255 [03:06<00:03,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 99%|█████████▉| 252/255 [03:06<00:02,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 99%|█████████▉| 253/255 [03:07<00:01,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|█████████▉| 254/255 [03:08<00:00,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|██████████| 255/255 [03:09<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "translated_ru = []\n",
    "for i in tqdm(range(len(test_ky))):\n",
    "        generated_sequences = generate_sequences(test_ky[i] + f' {delimiter} ', args)\n",
    "        translation = generated_sequences[1].strip().split('<pad>')[0]\n",
    "        translated_ru.append(translation.replace('.','').replace('_',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Все говорят, что нет денег, а ты возьми и купи слана',\n",
       " 'Это хамство! Дайте мне книгу!',\n",
       " 'Она живёт на улице Будановцев',\n",
       " 'Она живёт на улице Будавинко',\n",
       " 'Дайте, пожалуйста, гуляш с картофелем',\n",
       " '',\n",
       " 'Я купила два билета на поезд до Молодечно на львовском поезде',\n",
       " 'Я купила два билета до Молодечно на львовском поезде',\n",
       " 'Паспорт выдан Молодечненским РОВД Минской области 12 октября 2015 года и настоящий является 12 октября 2015 года',\n",
       " 'Этот сервер доступен только через наш VPN']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_ru[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Все говорят , что у них нет денег , а ты возьми и купи слона .',\n",
       " 'Это хамство ! Дайте мне книгу жалоб !',\n",
       " 'Она живёт на улице Будавников .',\n",
       " 'Она живёт на улице Строителей .',\n",
       " 'Дайте , пожалуйста , гуляш с картошкой .',\n",
       " 'Два билета до Минска , пожалуйста .',\n",
       " 'Я купила два билета до Молодечно на вильнюсском поезде .',\n",
       " 'Я купила два билета до Молодечно на вильнюсском поезде .',\n",
       " 'Паспорт выдан Молодечненским РОВД Минской области 12 марта 2015 года и действителен до 12 марта 2025 года .',\n",
       " 'Этот сервер доступен только через наш VPN .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ru[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.908630027379814"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sacrebleu.corpus_bleu(translated_ru[:], [test_ru[:]]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31055d0a0ddb47caafe8643fbce6fd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b1ea59d1934a8bb73aa302988f626a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.30 seconds, 839.90 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "P,R,F1 = score(translated_ru, test_ru, lang='ru', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8276)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8244)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8257)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "moses_predict = []\n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/TEST_TRANSLATED') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        moses_predict.append(line)  \n",
    "        \n",
    "        \n",
    "moses_true = []\n",
    "with open('/mnt/DATA2/grashchenkov/Checheny-Russki/be_ru/test.tok.ru') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        moses_true.append(line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Все говорят , что не имеют денег , а ты вазьмі и купи слона .',\n",
       " 'Это хамства ! Дайте мне книгу скарг !',\n",
       " 'Она живёт на улице Будаўнікоў .',\n",
       " 'Она живёт на улице Будаўнікоў .',\n",
       " 'Дайте , пожалуйста , гуляш с бульбай .',\n",
       " 'Два билеты к Мінска , пожалуйста .',\n",
       " 'Я купила два билеты к Молодечно на вільнюсскім поезде .',\n",
       " 'Я купила два билеты к Молодечно на вільнюсскім поезде .',\n",
       " 'Паспорт выдан Маладзечанскім РОВД Минской области двенадцать марта 2015 года и настоящий к двенадцать марта 2025 года .',\n",
       " 'Этот сервер даступны только из-за наш VPN .']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moses_predict[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Моя цель — сделать так , чтобы вы свободно заговорили на португальском языке и чтобы это было весело .',\n",
       " 'С твоего позволения .',\n",
       " 'Груша цвела последний год .',\n",
       " 'Мы с Петром часто ходим в кино .',\n",
       " 'Вы ему ничего не сказали ?',\n",
       " 'Франциск Скорина из Полоцка — восточнославянский первопечатник . Он принёс предкам белорусов , украинцев и русских технологию книги в 1517 году .',\n",
       " 'Хорошо . Продолжим !',\n",
       " 'Почему он поехал в Алжир ?',\n",
       " 'Этот сад красивый .',\n",
       " 'Протоны имеют положительный электрический заряд .']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moses_true[30:40]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/21/2021 11:26:36 - WARNING - sacrebleu -   That's 100 lines that end in a tokenized period ('.')\n",
      "10/21/2021 11:26:36 - WARNING - sacrebleu -   It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "10/21/2021 11:26:36 - WARNING - sacrebleu -   If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33.97506862176753"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sacrebleu.corpus_bleu(moses_predict[:], [moses_true[:]]).score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6899f0d9880447c2ba6126f12704927b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863f6ec09e1c499fac3ed238aed55dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.26 seconds, 999.19 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "P,R,F1 = score(moses_predict, moses_true, lang='ru', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8840)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8961)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8899)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert BlUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "import math\n",
    "bert_E = BertEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_list(embeddings, no_sep=False):\n",
    "    \"\"\"\n",
    "    Returns with the tokens of the embedding data from the BertEmbedding.\n",
    "\n",
    "    Params:\n",
    "        embeddings: The embedding data from BertEmbedding\n",
    "        no_sep: If True, the separators are trimmed.\n",
    "    Return:\n",
    "        tokens: list of tokens\n",
    "    \"\"\"\n",
    "    if no_sep:\n",
    "        return embeddings[0][0][1:-1]\n",
    "    return embeddings[0][0]\n",
    "\n",
    "\n",
    "def sentence_embs(embeddings):\n",
    "    \"\"\"Return with the sentence level embeddings\"\"\"\n",
    "    return embeddings[0][1][0]\n",
    "\n",
    "def prep(sentence):\n",
    "    \"\"\"Return with tokens and sentence level embeddings\"\"\"\n",
    "    embs = bert_E([sentence])\n",
    "    tokens = token_list(embs, no_sep=True)\n",
    "    se = sentence_embs(embs)\n",
    "    return tokens, se\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    numerator = sum(a*b for a,b in zip(x,y))\n",
    "    denominator = square_rooted(x)*square_rooted(y)\n",
    "    return numerator/float(denominator)\n",
    "\n",
    "def square_rooted(x):\n",
    "    return math.sqrt(sum([a*a for a in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = \"James Cook was a very good man and a loving husband.\"\n",
    "s1 = \"James Cook was a very nice man and a loving husband.\"\n",
    "s2 = \"James Cook was a bad man and a terrible husband.\"\n",
    "s3 = \"James Cook was a nice person and a good husband.\"\n",
    "s4 = \"The sky is blue today and learning history is important.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0, e0 = prep(s0)\n",
    "r1, e1 = prep(s1)\n",
    "r2, e2 = prep(s2)\n",
    "r3, e3 = prep(s3)\n",
    "r4, e4 = prep(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "SmoothingFunction = nltk.translate.bleu_score.SmoothingFunction()\n",
    "import nltk.translate.bleu_score as bleu\n",
    "from  nltk.translate.bleu_score import corpus_bleu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r0-r0 bleu score:  1.0\n",
      "r0-r1 bleu score:  0.6999271023161167\n",
      "r0-r2 bleu score:  0.3475075148610631\n",
      "r0-r3 bleu score:  0.29697089145035693\n",
      "r0-r4 bleu score:  0.10855926040543844\n"
     ]
    }
   ],
   "source": [
    "print(\"r0-r0 bleu score: \", bleu.sentence_bleu([r0], r0, smoothing_function=SmoothingFunction.method2))\n",
    "print(\"r0-r1 bleu score: \", bleu.sentence_bleu([r1], r0, smoothing_function=SmoothingFunction.method2))\n",
    "print(\"r0-r2 bleu score: \", bleu.sentence_bleu([r2], r0, smoothing_function=SmoothingFunction.method2))\n",
    "print(\"r0-r3 bleu score: \", bleu.sentence_bleu([r3], r0, smoothing_function=SmoothingFunction.method2))\n",
    "print(\"r0-r4 bleu score: \", bleu.sentence_bleu([r4], r0, smoothing_function=SmoothingFunction.method2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e0-e0 cosine-similarity: 1.0\n",
      "e0-e1 cosine-similarity: 0.9900622593588156\n",
      "e0-e2 cosine-similarity: 0.965961241983015\n",
      "e0-e3 cosine-similarity: 0.9760124353647561\n",
      "e0-e4 cosine-similarity: 0.30749654649663904\n"
     ]
    }
   ],
   "source": [
    "print(\"e0-e0 cosine-similarity:\", cosine_similarity(e0,e0))\n",
    "print(\"e0-e1 cosine-similarity:\", cosine_similarity(e1,e0))\n",
    "print(\"e0-e2 cosine-similarity:\", cosine_similarity(e2,e0))\n",
    "print(\"e0-e3 cosine-similarity:\", cosine_similarity(e3,e0))\n",
    "print(\"e0-e4 cosine-similarity:\", cosine_similarity(e4,e0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 255/255 [00:58<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "gpt_cosines = []\n",
    "moses_cosines = []\n",
    "for i in tqdm(range(len(translated_ru[:]))):#\n",
    "    try:\n",
    "        gpt_r0, gpt_e0 = prep(translated_ru[i])\n",
    "        test_r0, test_e0 = prep(test_ru[i])\n",
    "        moses_r0, moses_e0 = prep(moses_predict[i])\n",
    "\n",
    "        gpt_cosines.append(cosine_similarity(test_e0,gpt_e0))\n",
    "        moses_cosines.append(cosine_similarity(test_e0,moses_e0))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9563928450112729"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(moses_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9208955715238076"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gpt_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "# model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "# #embeddings = model.encode(sentences)\n",
    "\n",
    "# gpt_cosines_labse = []\n",
    "# moses_cosines_labse = []\n",
    "# for i in tqdm(range(260)):#len(translated_ru))\n",
    "#     try:\n",
    "#         gpt_embed = model.encode(translated_ru[i])\n",
    "#         test_embed = model.encode(test_ru[i])\n",
    "#         moses_embed = model.encode(moses_predict[i])\n",
    "\n",
    "#         gpt_cosines.append(cosine_similarity(test_embed,gpt_embed))\n",
    "#         moses_cosines.append(cosine_similarity(test_embed,moses_embed))\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9509852994308419"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(moses_cosines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515383112034733"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(gpt_cosines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850c9ad9a21948fd96f0b0d65748379f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b073f4abd8a4779a12006ec1a78ad0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9b6396063d4e39a498f2e85acf0a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig vs s1 0.9252627695789337\n",
      "Orig vs s2 0.9089925800423423\n"
     ]
    }
   ],
   "source": [
    "original = 'My mother washed the car'\n",
    "s_1 = 'Моя мать мыла машину'\n",
    "s_2 = 'My mother мыла car'\n",
    "\n",
    "orig_embed = model.encode(original)\n",
    "s1_embed = model.encode(s_1)\n",
    "s2_embed = model.encode(s_2)\n",
    "\n",
    "print('Orig vs s1',cosine_similarity(orig_embed,s1_embed))\n",
    "print('Orig vs s2',cosine_similarity(orig_embed,s2_embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c1701640ff46e2a342beb989525bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00da3f06197143d594149b89eb131b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.03 seconds, 29.19 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = score([s_1], [original], lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.6758]), tensor([0.8634]), tensor([0.7582]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, R, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6a6a9fcd2c43e28d79fed162475234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c4f12a0cc54295bf87b89802aa1c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.04 seconds, 27.52 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = score([s_2], [original], lang='en', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7891]), tensor([0.9188]), tensor([0.8491]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, R, F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
